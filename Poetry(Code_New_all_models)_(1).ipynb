{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpbYRWXrzmLc",
        "outputId": "51cec645-ccb5-49fb-ae4f-e0862231359b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: tensorflow\r\n",
            "Version: 2.0.4\r\n",
            "Summary: TensorFlow is an open source machine learning framework for everyone.\r\n",
            "Home-page: https://www.tensorflow.org/\r\n",
            "Author: Google Inc.\r\n",
            "Author-email: packages@tensorflow.org\r\n",
            "License: Apache 2.0\r\n",
            "Location: /home/aleef/anaconda3/envs/abdul/lib/python3.6/site-packages\r\n",
            "Requires: grpcio, gast, termcolor, numpy, keras-applications, protobuf, h5py, absl-py, wrapt, wheel, six, tensorflow-estimator, tensorboard, opt-einsum, google-pasta, astor, keras-preprocessing\r\n",
            "Required-by: \r\n"
          ]
        }
      ],
      "source": [
        "# !pip show tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "or8b9dGezmLu"
      },
      "outputs": [],
      "source": [
        "# !pip install google-cloud-translate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok\n",
        "!pip install flask_ngrok"
      ],
      "metadata": {
        "id": "PAnk2ocyC2zY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wTHfbiIzmLj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"./radiant-gateway-423216-t1-6a61a5b841e0.json\"\n",
        "# os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"/home/aleef/Downloads/radiant-gateway-423216-t1-6a61a5b841e0.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZ1Hot6WzmLp",
        "outputId": "08408be3-4b64-4aeb-f380-6e8f88a71361"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'translatedText': 'Bonjour le monde!', 'detectedSourceLanguage': 'en', 'input': 'Hello, world!'}\n"
          ]
        }
      ],
      "source": [
        "from google.cloud import translate_v2\n",
        "translate_client = translate_v2.Client()\n",
        "\n",
        "text = \"Hello, world!\"\n",
        "target = 'fr'  # French\n",
        "output = translate_client.translate(text, target_language=target)\n",
        "print(output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7iqLOZ_BzmLw"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow.keras.utils as ku\n",
        "from wordcloud import WordCloud\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow import keras\n",
        "from google.cloud import translate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0u0nqhr8zmLz"
      },
      "outputs": [],
      "source": [
        "def detect_language(text: str):\n",
        "    \"\"\"Detect the text\"\"\"\n",
        "    translate_client = translate.Client()\n",
        "    response = translate_client.detect_language(text)\n",
        "    return response['language']\n",
        "\n",
        "def translate_text(text, target_language):\n",
        "\n",
        "    translate_client = translate_v2.Client()\n",
        "\n",
        "    output = translate_client.translate(text, target_language=target_language)\n",
        "\n",
        "    return output['translatedText']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNeJaak_zmL1",
        "outputId": "4b48c4ed-66ca-4508-8606-3cc67a8ec0e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['mistra1' 'I have wished a bird would fly away,'\n",
            " 'And not sing by my house all day;' ...\n",
            " \"'Twas throwing words away; for still\"\n",
            " 'The little Maid would have her will,' 'And said, \"Nay, we are seven!\"']\n"
          ]
        }
      ],
      "source": [
        "ENG = pd.read_csv(\"English (Dataset).csv\", encoding='utf-8', header=None, sep=',', names=['mistra1'])\n",
        "Urdu = pd.read_csv(\"Poetry_Dataset_New.csv\", encoding='utf-8', header=None, sep=',', names=['mistra1'])\n",
        "Spanish = pd.read_csv(\"Spanish.csv\", encoding='utf-8', header=None, sep=',', names=['mistra1'])\n",
        "shayari= ENG[:2000]\n",
        "print(shayari['mistra1'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nzp41_CzmL4",
        "outputId": "67f9aae2-c47c-4990-f168-80b19dc438da"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1843"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "len(shayari[:2000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCUMdnEgzmL6",
        "outputId": "7628855d-5af9-4f90-8fb0-b26b9ff1b878"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Words: 3065\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Fitting the Tokenizer on the Corpus\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(shayari['mistra1'])\n",
        "\n",
        "# Vocabulary count of the corpus\n",
        "total_words = len(tokenizer.word_index)\n",
        "print(\"Total Words:\", total_words)\n",
        "\n",
        "# Converting the text into embeddings\n",
        "input_sequences = []\n",
        "for line in shayari['mistra1']:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "# Check if input_sequences is empty\n",
        "if input_sequences:\n",
        "    max_sequence_len = max([len(x) for x in input_sequences])\n",
        "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "    predictors, label = input_sequences[:, :-1], input_sequences[:, -1]\n",
        "    label = to_categorical(label, num_classes=total_words+1)\n",
        "else:\n",
        "    print(\"Error: No input sequences generated. Please check your corpus data.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b8LFEPyzmL9"
      },
      "source": [
        "# **Bi-Directional LSTM Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjUSaPwVzmMB",
        "outputId": "77693999-3651-407e-9713-520d1de47545"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 61, 100)           306600    \n",
            "                                                                 \n",
            " bidirectional (Bidirection  (None, 61, 300)           301200    \n",
            " al)                                                             \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 61, 300)           0         \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 100)               160400    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 3065)              309565    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 3066)              9400356   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 10478121 (39.97 MB)\n",
            "Trainable params: 10478121 (39.97 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "    # Building a Bi-Directional LSTM Model\n",
        "model_BiLSTM = Sequential()\n",
        "model_BiLSTM.add(Embedding(total_words+1, 100, input_length=max_sequence_len-1))\n",
        "model_BiLSTM.add(Bidirectional(LSTM(150, return_sequences=True)))\n",
        "model_BiLSTM.add(Dropout(0.2))\n",
        "model_BiLSTM.add(LSTM(100))\n",
        "model_BiLSTM.add(Dense(total_words+1/2, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
        "model_BiLSTM.add(Dense(total_words+1, activation='softmax'))\n",
        "model_BiLSTM.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
        "print(model_BiLSTM.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDO5HjyZzmME"
      },
      "source": [
        "# **LSTM Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5MnhRo1zmMF",
        "outputId": "44e79059-1413-4951-c65b-20dff70768ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 61, 100)           306600    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 61, 100)           0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 61, 3066)          309666    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 616266 (2.35 MB)\n",
            "Trainable params: 616266 (2.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# define the LSTM model\n",
        "model_lstm = Sequential()\n",
        "model_lstm.add(Embedding(total_words+1, 100, input_length=max_sequence_len-1))\n",
        "model_lstm.add(Dropout(0.2))\n",
        "model_lstm.add(Dense(total_words+1, activation='softmax'))\n",
        "model_lstm.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "print(model_lstm.summary())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQQBlM7CzmMH"
      },
      "source": [
        "# **3. Gated Recurrent Unit (GRU)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sCdcIg6zmMI",
        "outputId": "65e118e2-0d6e-41b1-d0f8-b1ea5605366c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 61, 100)           306600    \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirecti  (None, 256)               176640    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 3066)              787962    \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 3066)              9403422   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 10674624 (40.72 MB)\n",
            "Trainable params: 10674624 (40.72 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Parameters\n",
        "embedding_dim = 300\n",
        "gru_dim = 128\n",
        "dense_dim = total_words+1\n",
        "\n",
        "#define a model which used bidirection GRU, other details will be same as model_lstm\n",
        "\n",
        "# Model Definition with GRU\n",
        "model_gru = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(total_words+1, 100, input_length=max_sequence_len-1),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(units=gru_dim)),#add an LSTM layer where the no of units is equal to lstm_dim),\n",
        "    tf.keras.layers.Dense(dense_dim, activation='relu'),\n",
        "    tf.keras.layers.Dense(total_words+1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Set the training parameters\n",
        "# compile the above model for loss='binary_crossentropy',optimizer='adam',metrics=['accuracy']\n",
        "model_gru.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model_gru.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1r1RILeDzmMJ"
      },
      "source": [
        "# **4. Convolutional Neural Networks (CNNs)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kWXhrHzzmMK",
        "outputId": "89e3dcbc-49a4-4204-9762-adab23462e83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, 61, 100)           306600    \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 57, 128)           64128     \n",
            "                                                                 \n",
            " global_average_pooling1d (  (None, 128)               0         \n",
            " GlobalAveragePooling1D)                                         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 3066)              395514    \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 3066)              9403422   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 10169664 (38.79 MB)\n",
            "Trainable params: 10169664 (38.79 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "filters = 128\n",
        "kernel_size = 5\n",
        "dense_dim = total_words+1\n",
        "# Model Definition with Conv1D\n",
        "model_conv = tf.keras.Sequential([\n",
        "    # add embedding layer with vocab_size, embedding_dim, input_length=max_length,\n",
        "    #add Conv1D with filters, kernel_size, activation='relu' mentioned above,\n",
        "    tf.keras.layers.Embedding(total_words+1, 100, input_length=max_sequence_len-1),\n",
        "    tf.keras.layers.Conv1D(filters=filters, kernel_size=kernel_size),\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(dense_dim, activation='relu'),\n",
        "    tf.keras.layers.Dense(total_words+1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Set the training parameters\n",
        "# compile the above model for loss='binary_crossentropy',optimizer='adam',metrics=['accuracy']\n",
        "model_conv.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model_conv.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bB-jbJGpzmML"
      },
      "source": [
        "# **5.Flatten**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agQ2v0fOzmML",
        "outputId": "1460cd52-979d-4123-a901-aa199f5709d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_4 (Embedding)     (None, 61, 100)           306600    \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 6100)              0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 3066)              18705666  \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 3066)              0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 3066)              9403422   \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 3066)              9403422   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 37819110 (144.27 MB)\n",
            "Trainable params: 37819110 (144.27 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_4 (Embedding)     (None, 61, 100)           306600    \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 6100)              0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 3066)              18705666  \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 3066)              0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 3066)              9403422   \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 3066)              9403422   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 37819110 (144.27 MB)\n",
            "Trainable params: 37819110 (144.27 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Model Definition with a Flatten layer\n",
        "model_flatten = tf.keras.Sequential([\n",
        "    # Add embedding layer with vocab_size, embedding_dim, input_length=max_length,\n",
        "    tf.keras.layers.Embedding(total_words+1, 100, input_length=max_sequence_len-1),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(dense_dim, activation='relu'),\n",
        "    # tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(total_words+1, activation='relu'),\n",
        "    tf.keras.layers.Dense(total_words+1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_flatten.summary()\n",
        "# Set the training parameters\n",
        "\n",
        "# Add code below\n",
        "optimizer = keras.optimizers.Adam(lr=0.001)\n",
        "model_flatten.compile(loss='categorical_crossentropy',optimizer=optimizer,metrics=['categorical_accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model_flatten.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMk7NNX2zmMM"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(corpus):\n",
        "    global predictors, label, max_sequence_len, total_words, tokenizer  # Declare globals\n",
        "\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(corpus)\n",
        "\n",
        "    total_words = len(tokenizer.word_index)\n",
        "    print(\"Total Words:\", total_words)\n",
        "\n",
        "    input_sequences = []\n",
        "    for line in corpus:\n",
        "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "        for i in range(1, len(token_list)):\n",
        "            n_gram_sequence = token_list[:i+1]\n",
        "            input_sequences.append(n_gram_sequence)\n",
        "\n",
        "    if input_sequences:\n",
        "        max_sequence_len = max([len(x) for x in input_sequences])\n",
        "        input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "        predictors, label = input_sequences[:, :-1], input_sequences[:, -1]\n",
        "        label = to_categorical(label, num_classes=total_words+1)\n",
        "    else:\n",
        "        raise ValueError(\"No input sequences generated. Please check your corpus data.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KrUQkpa7zmMN"
      },
      "outputs": [],
      "source": [
        "def create_model_bilstm():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(total_words+1, 100, input_length=max_sequence_len-1))\n",
        "    model.add(Bidirectional(LSTM(150, return_sequences=True)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(LSTM(100))\n",
        "    model.add(Dense(total_words+1//2, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
        "    model.add(Dense(total_words+1, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    print(model.summary())\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QptR2AsOzmMN"
      },
      "outputs": [],
      "source": [
        "def create_model_lstm():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(total_words+1, 100, input_length=max_sequence_len-1))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(total_words+1, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "    print(model.summary())\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pE2ugRIwzmMO"
      },
      "outputs": [],
      "source": [
        "def create_model_gru():\n",
        "    embedding_dim = 300\n",
        "    gru_dim = 128\n",
        "    dense_dim = total_words + 1\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(total_words+1, 100, input_length=max_sequence_len-1),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.GRU(units=gru_dim)),\n",
        "        tf.keras.layers.Dense(dense_dim, activation='relu'),\n",
        "        tf.keras.layers.Dense(total_words+1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    print(model.summary())\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v52TJsOozmMO"
      },
      "outputs": [],
      "source": [
        "def create_model_conv():\n",
        "    filters = 128\n",
        "    kernel_size = 5\n",
        "    dense_dim = total_words + 1\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(total_words+1, 100, input_length=max_sequence_len-1),\n",
        "        tf.keras.layers.Conv1D(filters=filters, kernel_size=kernel_size),\n",
        "        tf.keras.layers.GlobalAveragePooling1D(),\n",
        "        tf.keras.layers.Dense(dense_dim, activation='relu'),\n",
        "        tf.keras.layers.Dense(total_words+1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    print(model.summary())\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OP9pFRkvzmMO"
      },
      "outputs": [],
      "source": [
        "def create_model_flatten():\n",
        "    dense_dim = total_words + 1\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(total_words+1, 100, input_length=max_sequence_len-1),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(dense_dim, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "        tf.keras.layers.Dense(total_words+1, activation='relu'),\n",
        "        tf.keras.layers.Dense(total_words+1, activation='sigmoid')\n",
        "    ])\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['categorical_accuracy'])\n",
        "    print(model.summary())\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5IvjFrBNzmMP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1__slfUjzmMP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lgheLnGzmMP"
      },
      "source": [
        "# **Model Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSwyzpsTzmMQ"
      },
      "outputs": [],
      "source": [
        "def BiLSTM_Model(text):\n",
        "    #Model Training\n",
        "    model = create_model_bilstm()\n",
        "    history_BiLSTM =  model.fit(predictors, label, epochs=1, verbose=1)\n",
        "    seed_text = text\n",
        "    next_words = 25\n",
        "    ouptut_text = \"\"\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences(\n",
        "            [token_list], maxlen=max_sequence_len-1,\n",
        "        padding='pre')\n",
        "        predicted = np.argmax( model.predict(token_list, verbose=0), axis=-1)\n",
        "        output_word = \"\"\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted:\n",
        "                output_word = word\n",
        "                break\n",
        "        seed_text += \" \" + output_word\n",
        "\n",
        "    return(seed_text)\n",
        "\n",
        "def LSTM_Model(text):\n",
        "    model = create_model_lstm()\n",
        "    #Model Training\n",
        "    history_lstm = model.fit(predictors, label, epochs=15, verbose=1)\n",
        "    seed_text = text\n",
        "    next_words = 15\n",
        "    ouptut_text = \"\"\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences(\n",
        "            [token_list], maxlen=max_sequence_len-1,\n",
        "        padding='pre')\n",
        "        predicted = np.argmax(model.predict(token_list, verbose=0), axis=-1)\n",
        "        output_word = \"\"\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted:\n",
        "                output_word = word\n",
        "                break\n",
        "        seed_text += \" \" + output_word\n",
        "\n",
        "    return(seed_text)\n",
        "\n",
        "def GRU_Model(text):\n",
        "    #Model Training\n",
        "    model = create_model_gru()\n",
        "    history_GRU = model.fit(predictors, label, epochs=1, verbose=1)\n",
        "    seed_text = text\n",
        "    next_words = 25\n",
        "    ouptut_text = \"\"\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences(\n",
        "            [token_list], maxlen=max_sequence_len-1,\n",
        "        padding='pre')\n",
        "        predicted = np.argmax(model.predict(token_list, verbose=0), axis=-1)\n",
        "        output_word = \"\"\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted:\n",
        "                output_word = word\n",
        "                break\n",
        "        seed_text += \" \" + output_word\n",
        "\n",
        "    return(seed_text)\n",
        "\n",
        "def CNN_Model(text):\n",
        "    #Model Training\n",
        "    model = create_model_conv()\n",
        "    history_CNN = model.fit(predictors, label, epochs=150, verbose=1)\n",
        "    seed_text = text\n",
        "    next_words = 25\n",
        "    ouptut_text = \"\"\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences(\n",
        "            [token_list], maxlen=max_sequence_len-1,\n",
        "        padding='pre')\n",
        "        predicted = np.argmax(model.predict(token_list, verbose=0), axis=-1)\n",
        "        output_word = \"\"\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted:\n",
        "                output_word = word\n",
        "                break\n",
        "        seed_text += \" \" + output_word\n",
        "\n",
        "    return(seed_text)\n",
        "\n",
        "def Flatten_Model(text):\n",
        "    #Model Training\n",
        "    model = create_model_conv()\n",
        "    history_CNN = model.fit(predictors, label, epochs=150, verbose=1)\n",
        "    seed_text = text\n",
        "    next_words = 25\n",
        "    ouptut_text = \"\"\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences(\n",
        "            [token_list], maxlen=max_sequence_len-1,\n",
        "        padding='pre')\n",
        "        predicted = np.argmax(model.predict(token_list, verbose=0), axis=-1)\n",
        "        output_word = \"\"\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted:\n",
        "                output_word = word\n",
        "                break\n",
        "        seed_text += \" \" + output_word\n",
        "\n",
        "    return(seed_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken 2ZqxT29XUucQ3NMIX4DLBcHF2Cq_4UFTmdxkanPGbwe9qCLNV"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62NACkaFB9BI",
        "outputId": "8eec29c8-088f-4eee-9cf0-2cde57bbb02d"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "public_url = ngrok.connect(5000)"
      ],
      "metadata": {
        "id": "XTEyjh9uB86X"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0GK2VzUmB8vY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "G4syu5KSzmMR"
      },
      "outputs": [],
      "source": [
        "from flask import Flask, render_template, request\n",
        "from werkzeug.serving import run_simple"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.serve_kernel_port_as_window(5000)"
      ],
      "metadata": {
        "id": "RVGm4pAk7VMu"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "sf6uVl6LzmMS"
      },
      "outputs": [],
      "source": [
        "from flask import Flask, request, render_template\n",
        "from flask_ngrok import run_with_ngrok\n",
        "\n",
        "app = Flask(__name__)\n",
        "run_with_ngrok(app)\n",
        "\n",
        "@app.route('/', methods=['GET', 'POST'])\n",
        "def index():\n",
        "    result = None\n",
        "    translated_result = None\n",
        "    if request.method == 'POST':\n",
        "        text = request.form['text']\n",
        "        model_type = request.form['model_type']\n",
        "        language = request.form['language']\n",
        "        translate_language = request.form['translate_language']\n",
        "\n",
        "\n",
        "        if model_type == 'BiLSTM':\n",
        "            result = BiLSTM_Model(text)\n",
        "        elif model_type == 'None':\n",
        "            None\n",
        "        elif model_type == 'GRU':\n",
        "            result = GRU_Model(text)\n",
        "        elif model_type == 'CNN':\n",
        "            result = CNN_Model(text)\n",
        "        elif model_type == 'Flatten':\n",
        "            result = Flatten_Model(text)\n",
        "        else:\n",
        "            result = \"Invalid model type\"\n",
        "        if language == 'english':\n",
        "            a = ENG[:1800]\n",
        "            preprocess_data(a['mistra1'])\n",
        "        elif language == 'None':\n",
        "            None\n",
        "        elif language == 'urdu':\n",
        "            a = Urdu[:1800]\n",
        "            preprocess_data(a['mistra1'])\n",
        "\n",
        "        elif language == 'spanish':\n",
        "            a = Spanish[:1800]\n",
        "            preprocess_data(a['mistra1'])\n",
        "\n",
        "\n",
        "\n",
        "        if translate_language == 'en':\n",
        "            translated_result = translate_text(text, 'en')\n",
        "        elif translate_language == 'None':\n",
        "            None\n",
        "        elif translate_language == 'ur':\n",
        "            translated_result = translate_text(text, 'ur')\n",
        "        elif translate_language == 'es':\n",
        "            translated_result = translate_text(text, 'es')\n",
        "        elif translate_language == 'fr':\n",
        "            translated_result = translate_text(text, 'fr')\n",
        "\n",
        "\n",
        "        return render_template('index.html', result=result, translated_result=translated_result)\n",
        "    else:\n",
        "        return render_template('index.html')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmu81-wlzmMT",
        "outputId": "f5e51928-7bcd-43a3-9efd-4a14315c52fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Running on http://5782-35-237-57-30.ngrok-free.app\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [14/May/2024 12:18:44] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [14/May/2024 12:18:45] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 61, 100)           304600    \n",
            "                                                                 \n",
            " bidirectional (Bidirection  (None, 61, 300)           301200    \n",
            " al)                                                             \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 61, 300)           0         \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 100)               160400    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 3045)              307545    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 3046)              9278116   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 10351861 (39.49 MB)\n",
            "Trainable params: 10351861 (39.49 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "135/374 [=========>....................] - ETA: 2:15 - loss: 7.3126 - accuracy: 0.0405"
          ]
        }
      ],
      "source": [
        "app.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "RlAacR9czmMT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c6f64277-382c-4ffb-f445-272dc8d5f302"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, text, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port);\n",
              "    const anchor = document.createElement('a');\n",
              "    anchor.href = new URL(path, url).toString();\n",
              "    anchor.target = '_blank';\n",
              "    anchor.setAttribute('data-href', url + path);\n",
              "    anchor.textContent = text;\n",
              "    element.appendChild(anchor);\n",
              "  })(5000, \"/\", \"https://localhost:5000/\", window.element)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab import output\n",
        "output.serve_kernel_port_as_window(5000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2f8LsMdCzmMU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "cvM6tqT7zmMU",
        "outputId": "3ad98808-9a9a-414d-8792-256f19c7373c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train on 224428 samples\n",
            "    32/224428 [..............................] - ETA: 32:03:37"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception in Tkinter callback\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/aleef/anaconda3/envs/abdul/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 2383, in get_attr\n",
            "    c_api.TF_OperationGetAttrValueProto(self._c_op, name, buf)\n",
            "tensorflow.python.framework.errors_impl.InvalidArgumentError: Operation 'while' has no attr named '_XlaCompile'.\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/aleef/anaconda3/envs/abdul/lib/python3.6/site-packages/tensorflow_core/python/ops/gradients_util.py\", line 345, in _MaybeCompile\n",
            "    xla_compile = op.get_attr(\"_XlaCompile\")\n",
            "  File \"/home/aleef/anaconda3/envs/abdul/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 2387, in get_attr\n",
            "    raise ValueError(str(e))\n",
            "ValueError: Operation 'while' has no attr named '_XlaCompile'.\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/aleef/anaconda3/envs/abdul/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 2383, in get_attr\n",
            "    c_api.TF_OperationGetAttrValueProto(self._c_op, name, buf)\n",
            "tensorflow.python.framework.errors_impl.InvalidArgumentError: Operation 'Sigmoid' has no attr named '_XlaCompile'.\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/aleef/anaconda3/envs/abdul/lib/python3.6/site-packages/tensorflow_core/python/ops/gradients_util.py\", line 345, in _MaybeCompile\n",
            "    xla_compile = op.get_attr(\"_XlaCompile\")\n",
            "  File \"/home/aleef/anaconda3/envs/abdul/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 2387, in get_attr\n",
            "    raise ValueError(str(e))\n",
            "ValueError: Operation 'Sigmoid' has no attr named '_XlaCompile'.\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/aleef/anaconda3/envs/abdul/lib/python3.6/tkinter/__init__.py\", line 1705, in __call__\n",
            "    return self.func(*args)\n",
            "  File \"<ipython-input-10-1b67b5805752>\", line 25, in BiLSTM_MODEL\n",
            "    result=BiLSTM_Model(text)\n",
            "  File \"<ipython-input-9-9021bf6eb247>\", line 3, in BiLSTM_Model\n",
            "    history_BiLSTM = model_BiLSTM.fit(predictors, label, epochs=1, verbose=1)\n",
            "  File \"/home/aleef/anaconda3/envs/abdul/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\", line 728, in fit\n",
            "    use_multiprocessing=use_multiprocessing)\n",
            "  File \"/home/aleef/anaconda3/envs/abdul/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 324, in fit\n",
            "    total_epochs=epochs)\n",
            "  File \"/home/aleef/anaconda3/envs/abdul/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 123, in run_one_epoch\n",
            "    batch_outs = execution_function(iterator)\n",
            "  File \"/home/aleef/anaconda3/envs/abdul/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 86, in execution_function\n",
            "    distributed_function(input_fn))\n",
            "  File \"/home/aleef/anaconda3/envs/abdul/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 457, in __call__\n",
            "    result = self._call(*args, **kwds)\n",
            "  File \"/home/aleef/anaconda3/envs/abdul/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 520, in _call\n",
            "    return self._stateless_fn(*args, **kwds)\n",
            "  File \"/home/aleef/anaconda3/envs/abdul/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1822, in __call__\n",
            "    graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\n",
            "  File \"/home/aleef/anaconda3/envs/abdul/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2150, in _maybe_define_function\n",
            "    graph_function = self._create_graph_function(args, kwargs)\n",
            "  File \"/home/aleef/anaconda3/envs/abdul/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2041, in _create_graph_function\n",
            "    capture_by_value=self._capture_by_value),\n",
            "  File \"/home/aleef/anaconda3/envs/abdul/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\", line 915, in func_graph_from_py_func\n",
            "    func_outputs = python_func(*func_args, **func_kwargs)\n",
            "  File \"/home/aleef/anaconda3/envs/abdul/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 358, in wrapped_fn\n",
            "    return weak_wrapped_fn().__wrapped__(*args, **kwds)\n",
            "  File \"/home/aleef/anaconda3/envs/abdul/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 73, in distributed_function\n",
            "    per_replica_function, args=(model, x, y, sample_weights))\n",
            "  File \"/home/aleef/anaconda3/envs/abdul/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 760, in experimental_run_v2\n",
            "    return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n",
            "  File \"/home/aleef/anaconda3/envs/abdul/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 1787, in call_for_each_replica\n",
            "    return self._call_for_each_replica(fn, args, kwargs)\n",
            "  File \"/home/aleef/anaconda3/envs/abdul/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 2132, in _call_for_each_replica\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/home/aleef/anaconda3/envs/abdul/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py\", line 292, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/aleef/anaconda3/envs/abdul/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 264, in train_on_batch\n",
            "    output_loss_metrics=model._output_loss_metrics)\n",
            "  File \"/home/aleef/anaconda3/envs/abdul/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_eager.py\", line 311, in train_on_batch\n",
            "    output_loss_metrics=output_loss_metrics))\n",
            "  File \"/home/aleef/anaconda3/envs/abdul/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_eager.py\", line 268, in _process_single_batch\n",
            "    grads = tape.gradient(scaled_total_loss, trainable_weights)\n",
            "  File \"/home/aleef/anaconda3/envs/abdul/lib/python3.6/site-packages/tensorflow_core/python/eager/backprop.py\", line 1014, in gradient\n",
            "    unconnected_gradients=unconnected_gradients)\n",
            "  File \"/home/aleef/anaconda3/envs/abdul/lib/python3.6/site-packages/tensorflow_core/python/eager/imperative_grad.py\", line 76, in imperative_grad\n",
            "    compat.as_str(unconnected_gradients.value))\n",
            "  File \"/home/aleef/anaconda3/envs/abdul/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 738, in _backward_function\n",
            "    return self._rewrite_forward_and_call_backward(call_op, *args)\n",
            "  File \"/home/aleef/anaconda3/envs/abdul/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 661, in _rewrite_forward_and_call_backward\n",
            "    forward_function, backwards_function = self.forward_backward(len(doutputs))\n",
            "  File \"/home/aleef/anaconda3/envs/abdul/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 582, in forward_backward\n",
            "    forward, backward = self._construct_forward_backward(num_doutputs)\n",
            "  File \"/home/aleef/anaconda3/envs/abdul/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 629, in _construct_forward_backward\n",
            "    func_graph=backwards_graph)\n",
            "  File \"/home/aleef/anaconda3/envs/abdul/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\", line 915, in func_graph_from_py_func\n",
            "    func_outputs = python_func(*func_args, **func_kwargs)\n",
            "  File \"/home/aleef/anaconda3/envs/abdul/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 619, in _backprop_function\n",
            "    src_graph=self._func_graph)\n",
            "  File \"/home/aleef/anaconda3/envs/abdul/lib/python3.6/site-packages/tensorflow_core/python/ops/gradients_util.py\", line 679, in _GradientsHelper\n",
            "    lambda: grad_fn(op, *out_grads))\n",
            "  File \"/home/aleef/anaconda3/envs/abdul/lib/python3.6/site-packages/tensorflow_core/python/ops/gradients_util.py\", line 350, in _MaybeCompile\n",
            "    return grad_fn()  # Exit early\n",
            "  File \"/home/aleef/anaconda3/envs/abdul/lib/python3.6/site-packages/tensorflow_core/python/ops/gradients_util.py\", line 679, in <lambda>\n",
            "    lambda: grad_fn(op, *out_grads))\n",
            "  File \"/home/aleef/anaconda3/envs/abdul/lib/python3.6/site-packages/tensorflow_core/python/ops/while_v2.py\", line 364, in _WhileGrad\n",
            "    util.unique_grad_fn_name(body_graph.name), op, maximum_iterations)\n",
            "  File \"/home/aleef/anaconda3/envs/abdul/lib/python3.6/site-packages/tensorflow_core/python/ops/while_v2.py\", line 607, in _create_grad_func\n",
            "    body_graph_inputs, body_graph_outputs))\n",
            "  File \"/home/aleef/anaconda3/envs/abdul/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\", line 915, in func_graph_from_py_func\n",
            "    func_outputs = python_func(*func_args, **func_kwargs)\n",
            "  File \"/home/aleef/anaconda3/envs/abdul/lib/python3.6/site-packages/tensorflow_core/python/ops/while_v2.py\", line 603, in <lambda>\n",
            "    lambda *args: _grad_fn(ys, xs, args, body_graph),\n",
            "  File \"/home/aleef/anaconda3/envs/abdul/lib/python3.6/site-packages/tensorflow_core/python/ops/while_v2.py\", line 663, in _grad_fn\n",
            "    unconnected_gradients=\"zero\")\n",
            "  File \"/home/aleef/anaconda3/envs/abdul/lib/python3.6/site-packages/tensorflow_core/python/ops/gradients_util.py\", line 679, in _GradientsHelper\n",
            "    lambda: grad_fn(op, *out_grads))\n",
            "  File \"/home/aleef/anaconda3/envs/abdul/lib/python3.6/site-packages/tensorflow_core/python/ops/gradients_util.py\", line 350, in _MaybeCompile\n",
            "    return grad_fn()  # Exit early\n",
            "  File \"/home/aleef/anaconda3/envs/abdul/lib/python3.6/site-packages/tensorflow_core/python/ops/gradients_util.py\", line 679, in <lambda>\n",
            "    lambda: grad_fn(op, *out_grads))\n",
            "  File \"/home/aleef/anaconda3/envs/abdul/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py\", line 966, in _SigmoidGrad\n",
            "    return gen_math_ops.sigmoid_grad(y, grad)\n",
            "  File \"/home/aleef/anaconda3/envs/abdul/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_math_ops.py\", line 9642, in sigmoid_grad\n",
            "    \"SigmoidGrad\", y=y, dy=dy, name=name)\n",
            "  File \"/home/aleef/anaconda3/envs/abdul/lib/python3.6/site-packages/tensorflow_core/python/framework/op_def_library.py\", line 793, in _apply_op_helper\n",
            "    op_def=op_def)\n",
            "  File \"/home/aleef/anaconda3/envs/abdul/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\", line 548, in create_op\n",
            "    compute_device)\n",
            "  File \"/home/aleef/anaconda3/envs/abdul/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 3429, in _create_op_internal\n",
            "    op_def=op_def)\n",
            "  File \"/home/aleef/anaconda3/envs/abdul/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 1759, in __init__\n",
            "    self._control_flow_context = self.graph._get_control_flow_context()\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "# #Import tkinter library\n",
        "# from tkinter import *\n",
        "# import tkinter as tk\n",
        "# import tkinter as ttk\n",
        "# #Create an instance of Tkinter frame or window\n",
        "# win= Tk()\n",
        "# #Set the geometry of tkinter frame\n",
        "# win.geometry(\"1050x750\")\n",
        "# Label(win, text=\"Welcome to Peotry Generation\", font=('Century 20 bold')).pack(pady=20)\n",
        "# def LSTM_MODEL():\n",
        "#    # print(\"Text\" + inputtxt.get())# Textbox widget\n",
        "#     #text = name.get() # now we have a str\n",
        "#     text = inputtxt.get(\"1.0\", \"end-1c\")\n",
        "#     result=BiLSTM_Model(text)\n",
        "#     Output = Text(win, height = 15,\n",
        "#               width = 30,\n",
        "#               bg = \"light cyan\")\n",
        "#     Output.pack()\n",
        "#     Output.insert(END, result)\n",
        "\n",
        "# def BiLSTM_MODEL():\n",
        "#    # print(\"Text\" + inputtxt.get())# Textbox widget\n",
        "#     #text = name.get() # now we have a str\n",
        "#     text = inputtxt.get(\"1.0\", \"end-1c\")\n",
        "#     result=BiLSTM_Model(text)\n",
        "#     Output = Text(win, height = 15,\n",
        "#               width = 30,\n",
        "#               bg = \"light cyan\")\n",
        "#     Output.pack()\n",
        "#     Output.insert(END, result)\n",
        "\n",
        "# def GRU_MODEL():\n",
        "#    # print(\"Text\" + inputtxt.get())# Textbox widget\n",
        "#     #text = name.get() # now we have a str\n",
        "#     text = inputtxt.get(\"1.0\", \"end-1c\")\n",
        "#     result=GRU_Model(text)\n",
        "#     Output = Text(win, height = 15,\n",
        "#               width = 30,\n",
        "#               bg = \"light cyan\")\n",
        "#     Output.pack()\n",
        "#     Output.insert(END, result)\n",
        "\n",
        "# def CNN_MODEL():\n",
        "#    # print(\"Text\" + inputtxt.get())# Textbox widget\n",
        "#     #text = name.get() # now we have a str\n",
        "#     text = inputtxt.get(\"1.0\", \"end-1c\")\n",
        "#     result=BiLSTM_Model(text)\n",
        "#     Output = Text(win, height = 15,\n",
        "#               width = 30,\n",
        "#               bg = \"light cyan\")\n",
        "#     Output.pack()\n",
        "#     Output.insert(END, result)\n",
        "\n",
        "# def Flatten_MODEL():\n",
        "#    # print(\"Text\" + inputtxt.get())# Textbox widget\n",
        "#     #text = name.get() # now we have a str\n",
        "#     text = inputtxt.get(\"1.0\", \"end-1c\")\n",
        "#     result=Flatten_Model(text)\n",
        "#     Output = Text(win, height = 15,\n",
        "#               width = 30,\n",
        "#               bg = \"light cyan\")\n",
        "#     Output.pack()\n",
        "#     Output.insert(END, result)\n",
        "\n",
        "\n",
        "# lbl = ttk.Label(win, text = \"Please Enter Text\").pack(pady=10)\n",
        "\n",
        "# inputtxt = Text(win, height = 10,\n",
        "#                 width = 25,\n",
        "#                 bg = \"light yellow\")\n",
        "# inputtxt.pack()\n",
        "# # Dropdown menu options\n",
        "# options = [\n",
        "# \t\"Urdu\",\n",
        "# \t\"English\",\n",
        "# \t\"Spanish\"\n",
        "# ]\n",
        "\n",
        "# # datatype of menu text\n",
        "# clicked = StringVar()\n",
        "\n",
        "# # initial menu text\n",
        "# clicked.set( \"Please Select Your Langugae\" )\n",
        "\n",
        "# # Create Dropdown menu\n",
        "# drop = OptionMenu( win , clicked , *options )\n",
        "# drop.pack()\n",
        "\n",
        "# #Create a Label and a Button widget\n",
        "# lbl = ttk.Label(win, text = \"Press Button to get Output\").pack(pady=10)\n",
        "\n",
        "\n",
        "# btn=Button(win, text=\"Bi_LSTM\", command= BiLSTM_MODEL)\n",
        "# btn.pack(ipadx=10)\n",
        "\n",
        "\n",
        "\n",
        "# lbl = ttk.Label(win, text = \"Output\").pack(pady=5)\n",
        "\n",
        "# win.bind('<Return>',lambda event:BiLSTM_MODEL())\n",
        "\n",
        "# win.mainloop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqbY3kuMzmMW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqBBrBpLzmMW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hogzFxZQzmMW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}